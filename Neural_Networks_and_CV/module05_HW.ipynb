{"cells":[{"cell_type":"code","source":["# 5.1.5\n","import numpy as np\n","def conv(x, kernel, pad, stride):\n","  new_x = np.zeros(x.shape + np.array([pad*2,pad*2]))\n","  if pad != 0:\n","    new_x[pad:-pad,pad:-pad] = x\n","  else:\n","    new_x = x\n","  # print(new_x)\n","\n","  res = []\n","  n_row = kernel.shape[0]\n","  n_col = kernel.shape[1]\n","  for i in range(0, new_x.shape[0] - n_row + 1, stride):\n","    for j in range(0, new_x.shape[1] - n_col + 1, stride):\n","      # print(i, j)\n","      res.append((new_x[i:i+n_row, j:j+n_col] * kernel).sum())\n","      # print(new_x[i:i+n_row, j:j+n_col])\n","      # print((new_x[i:i+n_row, j:j+n_col] * kernel).sum())\n","  return res\n","\n","# x = np.array([[1,1,1,0,0],[0,1,1,1,0],[0,0,1,1,1],[0,0,1,1,0],[0,1,1,0,0]])\n","# kernel = np.array([[1, 0, 1],[0, 1, 0],[1, 0, 1]])\n","# pad = 0\n","# stride = 1\n","\n","x = np.array([[4, 2, -1],[-6, 0, 5],[3, 2, 2]])\n","kernel = np.array([[0, 1, 2],[1, -1, 0],[1, 0, -2]])\n","pad = 1\n","stride = 2\n","\n","conv(x, kernel, pad, stride)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H3u5NuJsi8Tf","executionInfo":{"status":"ok","timestamp":1649882669810,"user_tz":-180,"elapsed":837,"user":{"displayName":"Миша Сотула","userId":"04023044834047779379"}},"outputId":"87a97fef-c4ec-4664-b463-d4a3a7990f78"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[-4.0, 3.0, -9.0, 5.0]"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["# 5.2.2\n","import torch\n","\n","# Создаем входной массив из двух изображений RGB 3*3\n","input_images = torch.tensor(\n","      [[[[0,  1,  2],\n","         [3,  4,  5],\n","         [6,  7,  8]],\n","\n","        [[9, 10, 11],\n","         [12, 13, 14],\n","         [15, 16, 17]],\n","\n","        [[18, 19, 20],\n","         [21, 22, 23],\n","         [24, 25, 26]]],\n","\n","\n","       [[[27, 28, 29],\n","         [30, 31, 32],\n","         [33, 34, 35]],\n","\n","        [[36, 37, 38],\n","         [39, 40, 41],\n","         [42, 43, 44]],\n","\n","        [[45, 46, 47],\n","         [48, 49, 50],\n","         [51, 52, 53]]]])\n","\n","print(input_images.shape)\n","\n","def get_padding2d(input_images):\n","    pad = 1\n","    new_im = torch.zeros(list(torch.tensor(input_images.shape) + torch.tensor([0, 0, pad*2,pad*2])))\n","    # new_im = torch.zeros(input_images.shape + torch([0, 0, pad*2,pad*2]))\n","    for i in range(input_images.shape[0]):\n","      for j in range(input_images.shape[1]):\n","        new_im[i, j, pad:-pad,pad:-pad] = input_images[i, j, :, :]\n","    return new_im\n","\n","get_padding2d(input_images)\n","\n","# Проверка происходит автоматически вызовом следующего кода\n","# (раскомментируйте для самостоятельной проверки,\n","#  в коде для сдачи задания должно быть закомментировано):\n","# print(torch.allclose(get_padding2d(input_images), correct_padded_images))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7kGvWiYNoGYu","executionInfo":{"status":"ok","timestamp":1649884108367,"user_tz":-180,"elapsed":11,"user":{"displayName":"Миша Сотула","userId":"04023044834047779379"}},"outputId":"d1ffeb48-de74-4439-8323-85911bb49223"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 3, 3, 3])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[[[ 0.,  0.,  0.,  0.,  0.],\n","          [ 0.,  0.,  1.,  2.,  0.],\n","          [ 0.,  3.,  4.,  5.,  0.],\n","          [ 0.,  6.,  7.,  8.,  0.],\n","          [ 0.,  0.,  0.,  0.,  0.]],\n","\n","         [[ 0.,  0.,  0.,  0.,  0.],\n","          [ 0.,  9., 10., 11.,  0.],\n","          [ 0., 12., 13., 14.,  0.],\n","          [ 0., 15., 16., 17.,  0.],\n","          [ 0.,  0.,  0.,  0.,  0.]],\n","\n","         [[ 0.,  0.,  0.,  0.,  0.],\n","          [ 0., 18., 19., 20.,  0.],\n","          [ 0., 21., 22., 23.,  0.],\n","          [ 0., 24., 25., 26.,  0.],\n","          [ 0.,  0.,  0.,  0.,  0.]]],\n","\n","\n","        [[[ 0.,  0.,  0.,  0.,  0.],\n","          [ 0., 27., 28., 29.,  0.],\n","          [ 0., 30., 31., 32.,  0.],\n","          [ 0., 33., 34., 35.,  0.],\n","          [ 0.,  0.,  0.,  0.,  0.]],\n","\n","         [[ 0.,  0.,  0.,  0.,  0.],\n","          [ 0., 36., 37., 38.,  0.],\n","          [ 0., 39., 40., 41.,  0.],\n","          [ 0., 42., 43., 44.,  0.],\n","          [ 0.,  0.,  0.,  0.,  0.]],\n","\n","         [[ 0.,  0.,  0.,  0.,  0.],\n","          [ 0., 45., 46., 47.,  0.],\n","          [ 0., 48., 49., 50.,  0.],\n","          [ 0., 51., 52., 53.,  0.],\n","          [ 0.,  0.,  0.,  0.,  0.]]]])"]},"metadata":{},"execution_count":82}]},{"cell_type":"code","source":["# 5.2.3\n","import numpy as np\n","\n","\n","def calc_out_shape(input_matrix_shape, out_channels, kernel_size, stride, padding):\n","\n","  out_shape = [input_matrix_shape[0],\n","               out_channels,\n","              int((input_matrix_shape[2] + 2*padding - (kernel_size - 1) - 1) / stride + 1),\n","              int((input_matrix_shape[3] + 2*padding - (kernel_size - 1) - 1) / stride + 1)\n","               ]\n","\n","  return out_shape\n","\n","print(calc_out_shape(input_matrix_shape=[2, 3, 10, 10],\n","                   out_channels=10,\n","                   kernel_size=3,\n","                   stride=1,\n","                   padding=0))\n","print(np.array_equal(\n","    calc_out_shape(input_matrix_shape=[2, 3, 10, 10],\n","                   out_channels=10,\n","                   kernel_size=3,\n","                   stride=1,\n","                   padding=0),\n","    [2, 10, 8, 8]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5M3p_dEAxdCs","executionInfo":{"status":"ok","timestamp":1649961171943,"user_tz":-180,"elapsed":7,"user":{"displayName":"Миша Сотула","userId":"04023044834047779379"}},"outputId":"059603bb-7183-44af-8b27-29d885a00f22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2, 10, 8, 8]\n","True\n"]}]},{"cell_type":"code","source":["# 5.2.5\n","import torch\n","from abc import ABC, abstractmethod\n","\n","\n","def calc_out_shape(input_matrix_shape, out_channels, kernel_size, stride, padding):\n","    batch_size, channels_count, input_height, input_width = input_matrix_shape\n","    output_height = (input_height + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n","    output_width = (input_width + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n","\n","    return batch_size, out_channels, output_height, output_width\n","\n","\n","class ABCConv2d(ABC):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride):\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.kernel_size = kernel_size\n","        self.stride = stride\n","\n","    def set_kernel(self, kernel):\n","        self.kernel = kernel\n","\n","    @abstractmethod\n","    def __call__(self, input_tensor):\n","        pass\n","\n","\n","class Conv2d(ABCConv2d):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride):\n","        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n","                                      stride, padding=0, bias=False)\n","\n","    def set_kernel(self, kernel):\n","        self.conv2d.weight.data = kernel\n","\n","    def __call__(self, input_tensor):\n","        return self.conv2d(input_tensor)\n","\n","\n","def create_and_call_conv2d_layer(conv2d_layer_class, stride, kernel, input_matrix):\n","    out_channels = kernel.shape[0]\n","    in_channels = kernel.shape[1]\n","    kernel_size = kernel.shape[2]\n","\n","    layer = conv2d_layer_class(in_channels, out_channels, kernel_size, stride)\n","    layer.set_kernel(kernel)\n","\n","    return layer(input_matrix)\n","\n","def test_conv2d_layer(conv2d_layer_class, batch_size=2,\n","                      input_height=4, input_width=4, stride=2):\n","    kernel = torch.tensor(\n","                      [[[[0., 1, 0],\n","                         [1,  2, 1],\n","                         [0,  1, 0]],\n","\n","                        [[1, 2, 1],\n","                         [0, 3, 3],\n","                         [0, 1, 10]],\n","\n","                        [[10, 11, 12],\n","                         [13, 14, 15],\n","                         [16, 17, 18]]]])\n","\n","    in_channels = kernel.shape[1]\n","\n","    input_tensor = torch.arange(0, batch_size * in_channels *\n","                                input_height * input_width,\n","                                out=torch.FloatTensor()) \\\n","        .reshape(batch_size, in_channels, input_height, input_width)\n","\n","    custom_conv2d_out = create_and_call_conv2d_layer(\n","        conv2d_layer_class, stride, kernel, input_tensor)\n","    conv2d_out = create_and_call_conv2d_layer(\n","        Conv2d, stride, kernel, input_tensor)\n","\n","    return torch.allclose(custom_conv2d_out, conv2d_out) \\\n","             and (custom_conv2d_out.shape == conv2d_out.shape)\n","\n","def get_padding2d(input_images, pad):\n","    new_im = torch.zeros(list(torch.tensor(input_images.shape) + torch.tensor([0, 0, pad*2,pad*2])))\n","    for i in range(input_images.shape[0]):\n","      for j in range(input_images.shape[1]):\n","        if pad != 0:\n","          new_im[i, j, pad:-pad,pad:-pad] = input_images[i, j, :, :]\n","        else:\n","          new_im[i, j, :,:] =  input_images[i, j, :, :]\n","    return new_im\n","\n","# Сверточный слой через циклы.\n","class Conv2dLoop(ABCConv2d):\n","    def __call__(self, input_tensor):\n","        tens_with_pad = get_padding2d(input_tensor, pad=0)\n","        output_tensor = torch.zeros(calc_out_shape(input_tensor.shape, self.out_channels, self.kernel_size, self.stride, 0))\n","\n","        n_row = self.kernel_size\n","        n_col = self.kernel_size\n","\n","        conv_for_each_image = []\n","        for k in range(input_tensor.shape[0]): # for image \n","          conv_for_each_filter = []\n","          for k_ker in range(self.out_channels): # for filter \n","            conv_for_each_channel = []\n","            for s in range(self.in_channels): # for channel \n","              conv_per_row = []\n","              for i in range(0, tens_with_pad.shape[2] - n_row + 1, self.stride):\n","                conv_per_col = []\n","                for j in range(0, tens_with_pad.shape[3] - n_col + 1, self.stride):\n","                  conv_per_col.append((tens_with_pad[k, s, i:i+n_row, j:j+n_col] * self.kernel[k_ker, s, :, :]).sum())\n","                conv_per_row.append(conv_per_col)\n","              conv_for_each_channel.append(conv_per_row) \n","            conv_for_each_channel = torch.tensor(conv_for_each_channel)\n","            conv_squesed_to_one_channel = conv_for_each_channel.sum(dim=0, keepdim=True).tolist()[0]\n","            conv_for_each_filter.append(conv_squesed_to_one_channel)\n","          conv_for_each_image.append(conv_for_each_filter)\n","        \n","        conv_for_each_image = torch.tensor(conv_for_each_image)\n","        return conv_for_each_image\n","\n","# Корректность реализации определится в сравнении со стандартным слоем из pytorch.\n","# Проверка происходит автоматически вызовом следующего кода\n","# (раскомментируйте для самостоятельной проверки,\n","#  в коде для сдачи задания должно быть закомментировано):\n","# print(test_conv2d_layer(Conv2dLoop))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c5tLC344nqbn","executionInfo":{"status":"ok","timestamp":1650058026642,"user_tz":-180,"elapsed":9,"user":{"displayName":"Миша Сотула","userId":"04023044834047779379"}},"outputId":"a57e99de-1f59-4463-9f56-54f83d75f47f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"code","source":["# 5.2.6\n","import torch\n","from abc import ABC, abstractmethod\n","\n","\n","def calc_out_shape(input_matrix_shape, out_channels, kernel_size, stride, padding):\n","    batch_size, channels_counqt, input_height, input_width = input_matrix_shape\n","    output_height = (input_height + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n","    output_width = (input_width + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n","\n","    return batch_size, out_channels, output_height, output_width\n","\n","\n","class ABCConv2d(ABC):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride):\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.kernel_size = kernel_size\n","        self.stride = stride\n","\n","    def set_kernel(self, kernel):\n","        self.kernel = kernel\n","\n","    @abstractmethod\n","    def __call__(self, input_tensor):\n","        pass\n","\n","\n","class Conv2d(ABCConv2d):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride):\n","        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n","                                      stride, padding=0, bias=False)\n","\n","    def set_kernel(self, kernel):\n","        self.conv2d.weight.data = kernel\n","\n","    def __call__(self, input_tensor):\n","        return self.conv2d(input_tensor)\n","\n","\n","def create_and_call_conv2d_layer(conv2d_layer_class, stride, kernel, input_matrix):\n","    out_channels = kernel.shape[0]\n","    in_channels = kernel.shape[1]\n","    kernel_size = kernel.shape[2]\n","\n","    layer = conv2d_layer_class(in_channels, out_channels, kernel_size, stride)\n","    layer.set_kernel(kernel)\n","\n","    return layer(input_matrix)\n","\n","\n","def test_conv2d_layer(conv2d_layer_class, batch_size=2,\n","                      input_height=4, input_width=4, stride=2):\n","    kernel = torch.tensor(\n","                      [[[[0., 1, 0],\n","                         [1,  2, 1],\n","                         [0,  1, 0]],\n","\n","                        [[1, 2, 1],\n","                         [0, 3, 3],\n","                         [0, 1, 10]],\n","\n","                        [[10, 11, 12],\n","                         [13, 14, 15],\n","                         [16, 17, 18]]]])\n","\n","    in_channels = kernel.shape[1]\n","\n","    input_tensor = torch.arange(0, batch_size * in_channels *\n","                                input_height * input_width,\n","                                out=torch.FloatTensor()) \\\n","        .reshape(batch_size, in_channels, input_height, input_width)\n","\n","    \n","    conv2d_out = create_and_call_conv2d_layer(\n","        Conv2d, stride, kernel, input_tensor)\n","    \n","    print('-------------')\n","    print(conv2d_out.shape)\n","    print(conv2d_out)\n","    print('-------------')\n","\n","    custom_conv2d_out = create_and_call_conv2d_layer(\n","        conv2d_layer_class, stride, kernel, input_tensor)\n","\n","    return torch.allclose(custom_conv2d_out, conv2d_out) \\\n","             and (custom_conv2d_out.shape == conv2d_out.shape)\n","\n","class Conv2dMatrix(ABCConv2d):\n","    # Функция преобразование кернела в матрицу нужного вида.\n","    def _unsqueeze_kernel(self, torch_input, output_height, output_width):\n","        print(torch_input.shape)\n","        m = torch.nn.ConstantPad1d((0, 1, 0, 1), 0)\n","        kernel_with_zeros = m(self.kernel)\n","        return kernel_with_zeros.flatten().unsqueeze(dim=0)\n","\n","    def __call__(self, torch_input):\n","        batch_size, out_channels, output_height, output_width\\\n","            = calc_out_shape(\n","                input_matrix_shape=torch_input.shape,\n","                out_channels=self.kernel.shape[0],\n","                kernel_size=self.kernel.shape[2],\n","                stride=self.stride,\n","                padding=0)\n","\n","        kernel_unsqueezed = self._unsqueeze_kernel(torch_input, output_height, output_width)\n","        result = kernel_unsqueezed @ torch_input.view((batch_size, -1)).permute(1, 0)\n","        return result.permute(1, 0).view((batch_size, self.out_channels,\n","                                          output_height, output_width))\n","\n","# Проверка происходит автоматически вызовом следующего кода\n","# (раскомментируйте для самостоятельной проверки,\n","#  в коде для сдачи задания должно быть закомментировано):\n","print(test_conv2d_layer(Conv2dMatrix))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8W2xBtRtJ0UI","executionInfo":{"status":"ok","timestamp":1650306376569,"user_tz":-180,"elapsed":709,"user":{"displayName":"Миша Сотула","userId":"04023044834047779379"}},"outputId":"f72b6a56-e9da-42da-b00b-52cc171ad7b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-------------\n","torch.Size([2, 1, 1, 1])\n","tensor([[[[ 5252.]]],\n","\n","\n","        [[[12596.]]]], grad_fn=<MkldnnConvolutionBackward0>)\n","-------------\n","torch.Size([2, 3, 4, 4])\n","True\n"]}]},{"cell_type":"code","source":["# 5.2.7\n","import torch\n","from abc import ABC, abstractmethod\n","\n","\n","def calc_out_shape(input_matrix_shape, out_channels, kernel_size, stride, padding):\n","    batch_size, channels_count, input_height, input_width = input_matrix_shape\n","    output_height = (input_height + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n","    output_width = (input_width + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n","\n","    return batch_size, out_channels, output_height, output_width\n","\n","\n","class ABCConv2d(ABC):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride):\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.kernel_size = kernel_size\n","        self.stride = stride\n","\n","    def set_kernel(self, kernel):\n","        self.kernel = kernel\n","\n","    @abstractmethod\n","    def __call__(self, input_tensor):\n","        pass\n","\n","\n","def create_and_call_conv2d_layer(conv2d_layer_class, stride, kernel, input_matrix):\n","    out_channels = kernel.shape[0]\n","    in_channels = kernel.shape[1]\n","    kernel_size = kernel.shape[2]\n","\n","    layer = conv2d_layer_class(in_channels, out_channels, kernel_size, stride)\n","    layer.set_kernel(kernel)\n","\n","    return layer(input_matrix)\n","\n","\n","class Conv2d(ABCConv2d):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride):\n","        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n","                                      stride, padding=0, bias=False)\n","\n","    def set_kernel(self, kernel):\n","        self.conv2d.weight.data = kernel\n","\n","    def __call__(self, input_tensor):\n","        return self.conv2d(input_tensor)\n","\n","\n","def test_conv2d_layer(conv2d_layer_class, batch_size=2,\n","                      input_height=4, input_width=4, stride=2):\n","    kernel = torch.tensor(\n","                      [[[[0., 1, 0],\n","                         [1,  2, 1],\n","                         [0,  1, 0]],\n","\n","                        [[1, 2, 1],\n","                         [0, 3, 3],\n","                         [0, 1, 10]],\n","\n","                        [[10, 11, 12],\n","                         [13, 14, 15],\n","                         [16, 17, 18]]]])\n","\n","    in_channels = kernel.shape[1]\n","\n","    input_tensor = torch.arange(0, batch_size * in_channels *\n","                                input_height * input_width,\n","                                out=torch.FloatTensor()) \\\n","        .reshape(batch_size, in_channels, input_height, input_width)\n","\n","    custom_conv2d_out = create_and_call_conv2d_layer(\n","        conv2d_layer_class, stride, kernel, input_tensor)\n","    conv2d_out = create_and_call_conv2d_layer(\n","        Conv2d, stride, kernel, input_tensor)\n","\n","    return torch.allclose(custom_conv2d_out, conv2d_out) \\\n","             and (custom_conv2d_out.shape == conv2d_out.shape)\n","\n","\n","class Conv2dMatrixV2(ABCConv2d):\n","    # Функция преобразования кернела в нужный формат.\n","    def _convert_kernel(self):\n","        converted_kernel = self.kernel.flatten().unsqueeze(0)\n","        return converted_kernel\n","\n","    # Функция преобразования входа в нужный формат.\n","    def _convert_input(self, torch_input, output_height, output_width):\n","        batch_size, out_channels, output_height, output_width = calc_out_shape(\n","            torch_input.shape, self.out_channels, self.kernel_size, \n","            self.stride, padding=0)\n","        \n","        out_dim = [batch_size, out_channels, output_height, output_width]\n","        list_of_flatten_img = []\n","        for k in range(batch_size):\n","            image = torch_input[k]\n","            for l in range(out_channels):\n","                for i in range(0, output_height, self.stride):\n","                    for j in range(0, output_height, self.stride):\n","                        sl = image[:, i:i+self.kernel_size, j:j+self.kernel_size]\n","                        list_of_flatten_img.append(sl.flatten())\n","        list_of_flatten_img = torch.stack(list_of_flatten_img).transpose(0,1)\n","        return list_of_flatten_img\n","\n","\n","    def __call__(self, torch_input):\n","        batch_size, out_channels, output_height, output_width\\\n","            = calc_out_shape(\n","                input_matrix_shape=torch_input.shape,\n","                out_channels=self.kernel.shape[0],\n","                kernel_size=self.kernel.shape[2],\n","                stride=self.stride,\n","                padding=0)\n","\n","        converted_kernel = self._convert_kernel()\n","        converted_input = self._convert_input(torch_input, output_height, output_width)\n","\n","        conv2d_out_alternative_matrix_v2 = converted_kernel @ converted_input\n","        return conv2d_out_alternative_matrix_v2.transpose(1,0).view(torch_input.shape[0],\n","                                                     self.out_channels, \n","                                                     output_height,\n","                                                     output_width)\n","\n","# Проверка происходит автоматически вызовом следующего кода\n","# (раскомментируйте для самостоятельной проверки,\n","#  в коде для сдачи задания должно быть закомментировано):\n","print(test_conv2d_layer(Conv2dMatrixV2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iGJIATPg8c1K","executionInfo":{"status":"ok","timestamp":1650310276668,"user_tz":-180,"elapsed":325,"user":{"displayName":"Миша Сотула","userId":"04023044834047779379"}},"outputId":"ad424a4b-23bc-4b61-8964-40adc606b96c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[[ 0.,  1.,  0.],\n","          [ 1.,  2.,  1.],\n","          [ 0.,  1.,  0.]],\n","\n","         [[ 1.,  2.,  1.],\n","          [ 0.,  3.,  3.],\n","          [ 0.,  1., 10.]],\n","\n","         [[10., 11., 12.],\n","          [13., 14., 15.],\n","          [16., 17., 18.]]]])\n","tensor([[ 0.,  1.,  0.,  1.,  2.,  1.,  0.,  1.,  0.,  1.,  2.,  1.,  0.,  3.,\n","          3.,  0.,  1., 10., 10., 11., 12., 13., 14., 15., 16., 17., 18.]])\n","tensor([[ 0., 48.],\n","        [ 1., 49.],\n","        [ 2., 50.],\n","        [ 4., 52.],\n","        [ 5., 53.],\n","        [ 6., 54.],\n","        [ 8., 56.],\n","        [ 9., 57.],\n","        [10., 58.],\n","        [16., 64.],\n","        [17., 65.],\n","        [18., 66.],\n","        [20., 68.],\n","        [21., 69.],\n","        [22., 70.],\n","        [24., 72.],\n","        [25., 73.],\n","        [26., 74.],\n","        [32., 80.],\n","        [33., 81.],\n","        [34., 82.],\n","        [36., 84.],\n","        [37., 85.],\n","        [38., 86.],\n","        [40., 88.],\n","        [41., 89.],\n","        [42., 90.]])\n","True\n"]}]},{"cell_type":"code","source":["# 5.6.10\n","import torch\n","\n","N = 4\n","C = 3\n","C_out = 10\n","H = 8\n","W = 16\n","\n","x = torch.ones((N, C, H, W))\n","\n","# torch.Size([4, 10, 8, 16])\n","out1 = torch.nn.Conv2d(C, C_out, kernel_size=(3, 3), padding=1)(x)\n","# print(out1.shape) # для самопроверки\n","\n","# torch.Size([4, 10, 8, 16])\n","out2 = torch.nn.Conv2d(C, C_out, kernel_size=(5, 5), padding=2)(x)\n","# print(out2.shape) # для самопроверки\n","\n","# torch.Size([4, 10, 8, 16])\n","out3 = torch.nn.Conv2d(C, C_out, kernel_size=(7, 7), padding=3)(x)\n","# print(out3.shape) # для самопроверки\n","\n","# torch.Size([4, 10, 8, 16])\n","out4 = torch.nn.Conv2d(C, C_out, kernel_size=(9, 9), padding=4)(x)\n","# print(out4.shape) # для самопроверки\n","\n","# # torch.Size([4, 10, 8, 16])\n","out5 = torch.nn.Conv2d(C, C_out, kernel_size=(3, 5), padding=(1,2))(x)\n","# print(out5.shape) # для самопроверки\n","\n","# # torch.Size([4, 10, 22, 30])\n","out6 = torch.nn.Conv2d(C, C_out, kernel_size=(3, 3), padding=8)(x)\n","# print(out6.shape) # для самопроверки\n","\n","# # torch.Size([4, 10, 7, 15])\n","out7 = torch.nn.Conv2d(C, C_out, kernel_size=(4, 4), padding=1)(x)\n","# print(out7.shape) # для самопроверки\n","\n","# # torch.Size([4, 10, 9, 17])\n","out8 = torch.nn.Conv2d(C, C_out, kernel_size=(2, 2), padding=1)(x)\n","# print(out8.shape) # для самопроверки"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Wqb_eblBMCQ","executionInfo":{"status":"ok","timestamp":1650107045860,"user_tz":-180,"elapsed":312,"user":{"displayName":"Миша Сотула","userId":"04023044834047779379"}},"outputId":"a4ef6fa9-a335-48bb-92b4-d52d1a23a6a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 10, 8, 16])\n","torch.Size([4, 10, 8, 16])\n","torch.Size([4, 10, 8, 16])\n","torch.Size([4, 10, 8, 16])\n","torch.Size([4, 10, 8, 16])\n","torch.Size([4, 10, 22, 30])\n","torch.Size([4, 10, 7, 15])\n","torch.Size([4, 10, 9, 17])\n"]}]}],"metadata":{"colab":{"collapsed_sections":[],"name":"module05_HW.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":0}